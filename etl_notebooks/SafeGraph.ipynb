{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "with open(\"local.settings.json\") as f:\n",
    "    os.environ.update(json.load(f)[\"Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping\n",
    "import cuid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def wkt_to_geojson(wkt_str):\n",
    "    try:\n",
    "        return mapping(wkt.loads(wkt_str))\n",
    "    except Exception:  # For production, catch a more specific exception\n",
    "        return None\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.environ[\"SAFEGRAPH_CONNECTION_STRIMG\"])\n",
    "container_client = blob_service_client.get_container_client(os.environ[\"SAFEGRAPH_CONTAINER_NAME\"])\n",
    "blobs = [\n",
    "    blob for blob in container_client.list_blobs(name_starts_with=os.environ[\"SAFEGRAPH_BLOB_PREFIX\"])\n",
    "    if blob.name.endswith(\".gz\")\n",
    "]\n",
    "\n",
    "df = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\n",
    "                container_client.get_blob_client(blob).download_blob(),\n",
    "                compression=\"gzip\",\n",
    "                usecols=[0, 3, 10, 11, 12, 13, 17, 25],\n",
    "            )\n",
    "            .dropna(\n",
    "                subset=[\n",
    "                    \"location_name\",\n",
    "                    \"placekey\",\n",
    "                    \"street_address\",\n",
    "                    \"city\",\n",
    "                    \"region\",\n",
    "                    \"postal_code\",\n",
    "                    \"polygon_wkt\",\n",
    "                ]\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"location_name\": \"title\",\n",
    "                    \"placekey\": \"ESQID\",\n",
    "                    \"street_address\": \"street\",\n",
    "                    \"region\": \"state\",\n",
    "                    \"postal_code\": \"zipCode\",\n",
    "                    \"polygon_wkt\": \"polygon\",\n",
    "                    \"related_parking\": \"relatedGeoFrames\",\n",
    "                }\n",
    "            )\n",
    "            .assign(\n",
    "                id=lambda x: [cuid.cuid() for _ in range(len(x))],\n",
    "                polygon=lambda x: x[\"polygon\"].apply(\n",
    "                    lambda y: json.dumps(\n",
    "                        {\n",
    "                            \"type\": \"FeatureCollection\",\n",
    "                            \"features\": [\n",
    "                                {\"type\": \"Feature\", \"geometry\": wkt_to_geojson(y)}\n",
    "                            ],\n",
    "                        }\n",
    "                    )\n",
    "                ),\n",
    "                related_parking=lambda x: x[\"related_parking\"].apply(json.loads),\n",
    "                source=\"SafeGraph\",\n",
    "                zipCode=lambda x: x[\"zipCode\"].apply(lambda y: str(int(y)).zfill(5)),\n",
    "            )\n",
    "            for blob in blobs\n",
    "            if f.endswith(\".gz\")\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"title\",\n",
    "            \"source\",\n",
    "            \"ESQID\",\n",
    "            \"street\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"zipCode\",\n",
    "            \"polygon\",\n",
    "            \"relatedGeoFrames\"\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import urllib.parse\n",
    "\n",
    "dsn = os.environ[\"DATABIND_SQL_KEYSTONE\"]\n",
    "\n",
    "# Remove the SQLAlchemy prefix\n",
    "if dsn.startswith(\"postgresql+psycopg2://\"):\n",
    "    dsn = dsn.replace(\"postgresql+psycopg2://\", \"\")\n",
    "\n",
    "# Prepend a scheme for proper URL parsing\n",
    "parsed = urllib.parse.urlparse(\"postgresql://\" + dsn)\n",
    "\n",
    "csv_file = \"all_data.csv\"\n",
    "\n",
    "with psycopg2.connect(\n",
    "    dbname=parsed.path[1:],  # Remove leading slash\n",
    "    user=parsed.username,\n",
    "    password=parsed.password,\n",
    "    host=parsed.hostname,\n",
    "    port=parsed.port,\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # 1. Create a temporary staging table.\n",
    "        # Adjust the column types if necessary. Here we assume polygon is stored as JSONB.\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TEMP TABLE staging_table (\n",
    "                id TEXT,\n",
    "                source TEXT,\n",
    "                \"ESQID\" TEXT,\n",
    "                title TEXT,\n",
    "                street TEXT,\n",
    "                city TEXT,\n",
    "                state TEXT,\n",
    "                \"zipCode\" TEXT,\n",
    "                polygon JSONB\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "        print(\"Table created.\")\n",
    "\n",
    "        # 2. Load CSV data into the staging table using the COPY command.\n",
    "        with open(csv_file, \"r\") as f:\n",
    "            # Skip the header row if your CSV includes it.\n",
    "            next(f)\n",
    "            cur.copy_expert(\n",
    "                sql=\"\"\"\n",
    "                COPY staging_table (id, title, source, \"ESQID\", street, city, state, \"zipCode\", polygon)\n",
    "                FROM STDIN WITH CSV QUOTE '\\\"'\n",
    "                \"\"\",\n",
    "                file=f,\n",
    "            )\n",
    "        conn.commit()\n",
    "        print(\"Data copied\")\n",
    "\n",
    "        # 3. Upsert data into the target table.\n",
    "        # This statement inserts new records and updates existing rows based on the primary key (id).\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO keystone.\"TargetingGeoFrame\" (id, title, source, \"ESQID\", street, city, state, \"zipCode\", polygon)\n",
    "            SELECT DISTINCT ON (\"ESQID\") id, title, source, \"ESQID\", street, city, state, \"zipCode\", polygon\n",
    "            FROM staging_table\n",
    "            ORDER BY \"ESQID\", id  -- Adjust the order clause to pick the preferred record per ESQID\n",
    "            ON CONFLICT (\"ESQID\")\n",
    "            DO UPDATE SET\n",
    "                title = EXCLUDED.title,\n",
    "                street = EXCLUDED.street,\n",
    "                city = EXCLUDED.city,\n",
    "                state = EXCLUDED.state,\n",
    "                \"zipCode\" = EXCLUDED.\"zipCode\",\n",
    "                polygon = EXCLUDED.polygon;\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "print(\"Table updated successfully using the CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set environmental variables for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson, os\n",
    "with open(\"local.settings.json\") as f:\n",
    "    os.environ.update(orjson.loads(f.read())[\"Values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OnSpot Queue Stats\n",
    "Legacy mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.openapi.clients import OnSpotAPI\n",
    "OSA = OnSpotAPI(production=True)\n",
    "stat = OSA.createRequest((\"/status/queue\", \"get\"))\n",
    "_, data, _ = await stat.request()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register SQL data provider\n",
    "Note: this may take about 15-30 seconds to complete as it waits for the SQL database to be available and scans the provider metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.data import from_bind, register_binding\n",
    "\n",
    "if not from_bind(\"salesforce\"):\n",
    "    register_binding(\n",
    "        \"salesforce\",\n",
    "        \"Structured\",\n",
    "        \"sql\",\n",
    "        url=os.environ[\"DATABIND_SQL_SALESFORCE\"],\n",
    "        schemas=[\"dbo\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate data provider session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.data.structured.sqlalchemy import SQLAlchemyStructuredProvider\n",
    "\n",
    "provider: SQLAlchemyStructuredProvider = from_bind(\"salesforce\")\n",
    "tables = provider.models[\"dbo\"]\n",
    "session = provider.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query provider session for data (location geoframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "location_geoframes = [\n",
    "    (\n",
    "        row.Name,\n",
    "        geojson.loads(\n",
    "            row.JSON_String__c[:-1]\n",
    "            if row.JSON_String__c[-1] == \",\"\n",
    "            else row.JSON_String__c\n",
    "        ),\n",
    "    )\n",
    "    for row in session.query(\n",
    "        tables[\"GeoJSON_Location__c\"].Name,\n",
    "        tables[\"GeoJSON_Location__c\"].JSON_String__c,\n",
    "    )\n",
    "    .join(\n",
    "        tables[\"GeoJSON_Join__c\"],\n",
    "        tables[\"GeoJSON_Join__c\"].GeoJSON_Location__c\n",
    "        == tables[\"GeoJSON_Location__c\"].Id,\n",
    "    )\n",
    "    .join(\n",
    "        tables[\"Audience__c\"],\n",
    "        tables[\"Audience__c\"].Id == tables[\"GeoJSON_Join__c\"].Audience__c,\n",
    "    )\n",
    "    .join(\n",
    "        tables[\"Child_ID__c\"],\n",
    "        tables[\"Child_ID__c\"].Id == tables[\"Audience__c\"].Child_ID__c,\n",
    "    )\n",
    "    .join(\n",
    "        tables[\"Account\"],\n",
    "        tables[\"Account\"].Id == tables[\"Child_ID__c\"].Parent_Account__c,\n",
    "    )\n",
    "    .filter(\n",
    "        tables[\"Account\"].IsDeleted == False,\n",
    "        tables[\"Child_ID__c\"].IsDeleted == False,\n",
    "        tables[\"Audience__c\"].IsDeleted == False,\n",
    "        tables[\"GeoJSON_Join__c\"].IsDeleted == False,\n",
    "        tables[\"GeoJSON_Location__c\"].IsDeleted == False,\n",
    "        tables[\"Account\"].Account_Status__c == \"Active\",\n",
    "        tables[\"Audience__c\"].Active_Audience__c == True,\n",
    "        tables[\"Audience__c\"].Audience_Type__c == \"Competitor Location\",\n",
    "        tables[\"GeoJSON_Join__c\"].Active_Location__c == True,\n",
    "    )\n",
    "    .distinct()\n",
    "]\n",
    "len(location_geoframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in location_geoframes if f[0] == \"EF~33767\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "container: ContainerClient = ContainerClient.from_connection_string(\n",
    "    conn_str=os.environ[\"ONSPOT_CONN_STR\"], container_name=\"dashboard\"\n",
    ")\n",
    "\n",
    "location_geoframes[2] = location_geoframes[0].str.replace(\"~\", \"\")\n",
    "location_geoframes[\n",
    "    ~location_geoframes[2].isin(\n",
    "        [\n",
    "            p.split(\"/\")[-1]\n",
    "            for p in container.list_blob_names(\n",
    "                name_starts_with=\"raw/2428875ee1674f548d47f05fa633119d/observations/\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "][1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format the OnSpot requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import uuid, os\n",
    "\n",
    "# Relative date range\n",
    "# now = datetime.utcnow()\n",
    "# today = datetime(now.year, now.month, now.day)\n",
    "# end = today - relativedelta(days=2)\n",
    "# start = end - relativedelta(days=75)\n",
    "\n",
    "# Static data range\n",
    "start = datetime(2023,4,10)\n",
    "end = datetime(2023,4,19)\n",
    "\n",
    "requests = [\n",
    "    {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [\n",
    "            {\n",
    "                **value,\n",
    "                \"properties\": {\n",
    "                    \"name\": key,\n",
    "                    \"fileName\": key,\n",
    "                    \"start\": start.isoformat(),\n",
    "                    \"end\": end.isoformat(),\n",
    "                    \"hash\": False,\n",
    "                },\n",
    "            }\n",
    "            for key, value in location_geoframes\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "ingress = {\n",
    "    \"instance_id\": (instance_id := uuid.uuid4().hex),\n",
    "    \"conn_str\": \"ONSPOT_CONN_STR\"\n",
    "    if \"ONSPOT_CONN_STR\" in os.environ.keys()\n",
    "    else \"AzureWebJobsStorage\",\n",
    "    \"container\": os.environ.get(\"ONSPOT_CONTAINER\", \"dashboard\"),\n",
    "    \"outputPath\": f\"oneoff/{instance_id}/devices\",\n",
    "    \"endpoint\": \"/save/geoframe/all/devices\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inject callback and outputLocation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import (\n",
    "    ContainerClient,\n",
    "    ContainerSasPermissions,\n",
    "    generate_container_sas,\n",
    ")\n",
    "\n",
    "if ingress[\"endpoint\"].startswith(\"/save/\"):\n",
    "    container = ContainerClient.from_connection_string(\n",
    "        os.environ[ingress[\"conn_str\"]]\n",
    "        if ingress.get(\"conn_str\", None) in os.environ.keys()\n",
    "        else os.environ[\"AzureWebJobsStorage\"],\n",
    "        container_name=ingress.get(\"container\", \"general\"),\n",
    "    )\n",
    "    if not container.exists():\n",
    "        container.create_container()\n",
    "    sas_token = generate_container_sas(\n",
    "        account_name=container.credential.account_name,\n",
    "        account_key=container.credential.account_key,\n",
    "        container_name=container.container_name,\n",
    "        permission=ContainerSasPermissions(write=True, read=True),\n",
    "        expiry=datetime.utcnow() + relativedelta(days=2),\n",
    "    )\n",
    "event_url = \"https://webhook.site/e8d0f8b4-25a9-483f-8273-4dddf5508c67\"\n",
    "\n",
    "for request in requests:\n",
    "    if request.get(\"type\", None) == \"FeatureCollection\":\n",
    "        for feature in request[\"features\"]:\n",
    "            feature[\"properties\"][\"callback\"] = event_url.replace(\n",
    "                \"{eventName}\", uuid.uuid4().hex\n",
    "            )\n",
    "            if ingress[\"endpoint\"].startswith(\"/save/\"):\n",
    "                feature[\"properties\"][\"outputLocation\"] = (\n",
    "                    container.url.replace(\"https://\", \"az://\")\n",
    "                    + \"/{}?\".format(ingress.get(\"outputPath\", ingress[\"instance_id\"]))\n",
    "                    + sas_token\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.openapi.clients.onspot import OnSpot\n",
    "req = OnSpot[(ingress[\"endpoint\"], \"post\")]\n",
    "for request in requests:\n",
    "    display(req(request))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

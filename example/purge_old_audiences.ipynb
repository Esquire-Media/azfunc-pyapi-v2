{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson, os\n",
    "with open(\"local.settings.json\") as f:\n",
    "    os.environ.update(orjson.loads(f.read())[\"Values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg_code",
   "metadata": {},
   "outputs": [],
   "source": "from __future__ import annotations\n\nimport os\nimport json\nimport time\nfrom collections import deque\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nimport pandas as pd\nimport requests\nfrom azure.data.tables import TableClient\nfrom azure.core.pipeline.transport import RequestsTransport\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom functools import lru_cache\nfrom sqlalchemy import MetaData, Table, create_engine, select\n\n# Polling / concurrency\nPOLL_SECONDS = 10\nMAX_CONCURRENT = 10\n\n# Durable Functions endpoints\nDURABLE_BASE = \"https://esquire-auto-audience.azurewebsites.net\"\nTASK_HUB = \"production\"\nCONNECTION = \"Storage\"\nMASTER_CODE = os.environ.get(\"AZFUNC_MASTER_CODE\", \"\")\n\n# Storage table\nTABLE_NAME = \"productionInstances\"\nNAME_FILTER = \"orchestrator_esquire_audience\"\n\n# Safety switches\nDRY_RUN_DELETE = False\nDRY_RUN_START = False\n\npd.set_option(\"display.max_rows\", 200)\npd.set_option(\"display.max_colwidth\", 200)\n\n# Connection pooling for HTTP clients\n@lru_cache(maxsize=1)\ndef _get_shared_session() -> requests.Session:\n    \"\"\"Create a session with optimized connection pooling.\"\"\"\n    session = requests.Session()\n    retry_strategy = Retry(\n        total=3, backoff_factor=0.5,\n        status_forcelist=[429, 500, 502, 503, 504],\n        allowed_methods=[\"HEAD\", \"GET\", \"PUT\", \"POST\", \"DELETE\", \"OPTIONS\"]\n    )\n    adapter = HTTPAdapter(\n        pool_connections=10, pool_maxsize=20,\n        max_retries=retry_strategy, pool_block=False\n    )\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n@lru_cache(maxsize=1)\ndef _get_table_transport() -> RequestsTransport:\n    \"\"\"Shared transport for TableClient with connection pooling.\"\"\"\n    return RequestsTransport(\n        session=_get_shared_session(),\n        session_owner=False,\n        connection_timeout=60,\n        read_timeout=300\n    )"
  },
  {
   "cell_type": "markdown",
   "id": "db",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(os.environ[\"DATABIND_SQL_KEYSTONE\"])\n",
    "schema = \"keystone\"\n",
    "audience_table_name = \"Audience\"\n",
    "\n",
    "def fetch_audiences(engine) -> pd.DataFrame:\n",
    "    metadata = MetaData()\n",
    "    table = Table(audience_table_name, metadata, schema=schema, autoload_with=engine)\n",
    "    cols = [table.c[\"id\"], table.c[\"status\"]]\n",
    "    stmt = select(*cols)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(stmt)\n",
    "        rows = [row for row in result.mappings()]\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "audiences = fetch_audiences(engine)\n",
    "audience_ids = set(audiences.loc[audiences[\"status\"].astype(bool), \"id\"].tolist())\n",
    "print(f\"Active audiences in DB: {len(audience_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instances",
   "metadata": {},
   "source": [
    "# Instances table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instances_code",
   "metadata": {},
   "outputs": [],
   "source": "def _safe_json_loads(x: Any) -> Any:\n    if isinstance(x, dict) or x is None:\n        return x\n    if isinstance(x, (bytes, bytearray)):\n        try:\n            return json.loads(x)\n        except Exception:\n            return x\n    if isinstance(x, str):\n        try:\n            return json.loads(x)\n        except Exception:\n            return x\n    return x\n\ndef get_instances_from_connection_string(\n    conn_str: str,\n    name_filter: str = NAME_FILTER,\n) -> pd.DataFrame:\n    \"\"\"Query the instances table for orchestrator rows (server-side filtered).\n\n    Columns returned: PartitionKey, RuntimeStatus, CustomStatus, CreatedTime, CompletedTime\n    \"\"\"\n    table_client = TableClient.from_connection_string(\n        conn_str,\n        table_name=TABLE_NAME,\n        transport=_get_table_transport()\n    )\n\n    select_fields = [\n        \"PartitionKey\",\n        \"RuntimeStatus\",\n        \"CustomStatus\",\n        \"CreatedTime\",\n        \"CompletedTime\",\n    ]\n    odata_filter = f\"Name eq '{name_filter}'\"\n    entities = list(table_client.query_entities(select=select_fields, query_filter=odata_filter))\n    df = pd.DataFrame(entities)\n    if df.empty:\n        return df\n\n    if \"CustomStatus\" in df.columns:\n        df[\"CustomStatus\"] = df[\"CustomStatus\"].apply(_safe_json_loads)\n\n    if \"CreatedTime\" in df.columns:\n        df = df.sort_values(by=\"CreatedTime\", ascending=True).reset_index(drop=True)\n\n    return df\n\ndef get_instances() -> pd.DataFrame:\n    return get_instances_from_connection_string(os.getenv(\"AzureWebJobsStorage\", \"\"), NAME_FILTER)\n\ninstances = get_instances()\nprint(f\"Instances in table: {len(instances)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "helpers",
   "metadata": {},
   "source": [
    "# Helpers: running detection + Durable API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers_code",
   "metadata": {},
   "outputs": [],
   "source": "def _state_endswith_ellipsis(custom_status: Any) -> bool:\n    if not isinstance(custom_status, dict):\n        return False\n    state = custom_status.get(\"state\")\n    return isinstance(state, str) and state.endswith(\"...\")\n\ndef is_running_row(row: pd.Series) -> bool:\n    # If it has CompletedTime, it is not running (this includes errored/failed per your note).\n    if \"CompletedTime\" in row and pd.notna(row[\"CompletedTime\"]):\n        return False\n\n    runtime = row.get(\"RuntimeStatus\")\n    if runtime == \"Pending\":\n        return True\n\n    return _state_endswith_ellipsis(row.get(\"CustomStatus\"))\n\ndef running_instance_ids(df: pd.DataFrame) -> set[str]:\n    if df is None or df.empty:\n        return set()\n    tmp = df.copy()\n    tmp[\"_is_running\"] = tmp.apply(is_running_row, axis=1)\n    return set(tmp.loc[tmp[\"_is_running\"], \"PartitionKey\"].astype(str).tolist())\n\n# Shared session for Durable Functions API calls\n_shared_session = _get_shared_session()\n\ndef durable_delete(instance_id: str) -> requests.Response | None:\n    if DRY_RUN_DELETE:\n        print(f\"[DRY RUN] Would delete instance {instance_id}\")\n        return None\n    return _shared_session.delete(\n        url=f\"{DURABLE_BASE}/runtime/webhooks/durabletask/instances/{instance_id}\",\n        params={\n            \"taskHub\": TASK_HUB,\n            \"connection\": CONNECTION,\n            \"code\": MASTER_CODE,\n        },\n        timeout=30,\n    )\n\ndef durable_start(audience_id: str, force: int = 1) -> requests.Response | None:\n    if DRY_RUN_START:\n        print(f\"[DRY RUN] Would start audience {audience_id}\")\n        return None\n    return _shared_session.post(\n        url=f\"{DURABLE_BASE}/api/audiences/{audience_id}\",\n        params={\n            \"force\": force,\n            \"code\": MASTER_CODE,\n        },\n        timeout=30,\n    )"
  },
  {
   "cell_type": "markdown",
   "id": "compare",
   "metadata": {},
   "source": [
    "# Compare active audiences vs instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = get_instances()\n",
    "instance_ids = set(instances[\"PartitionKey\"].astype(str).tolist()) if len(instances) else set()\n",
    "\n",
    "absent_audiences = instance_ids - audience_ids          # instances that should be removed\n",
    "missing_instance_audiences = audience_ids - instance_ids # active audiences missing an instance row entirely\n",
    "\n",
    "print(f\"Instances in table: {len(instance_ids)}\")\n",
    "print(f\"Active audiences:   {len(audience_ids)}\")\n",
    "print(f\"Overlap:           {len(instance_ids & audience_ids)}\")\n",
    "print(f\"To delete:         {len(absent_audiences)}\")\n",
    "print(f\"To start:          {len(missing_instance_audiences)}\")\n",
    "if len(instance_ids):\n",
    "    print(f\"% removed:         {len(absent_audiences)/len(instance_ids):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete",
   "metadata": {},
   "source": [
    "# Delete instances for inactive audiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_id in sorted(absent_audiences):\n",
    "    print(f\"Deleting inactive instance: {instance_id}\")\n",
    "    resp = durable_delete(instance_id)\n",
    "    if resp is not None and resp.status_code >= 400:\n",
    "        print(f\"  ! delete failed: {resp.status_code} {resp.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start_missing",
   "metadata": {},
   "source": [
    "# Start missing active audiences (poll table every 10s, max 10 running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start_missing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_missing_with_concurrency(\n",
    "    ids_to_start: list[str],\n",
    "    max_concurrent: int = MAX_CONCURRENT,\n",
    "    poll_seconds: int = POLL_SECONDS,\n",
    ") -> set[str]:\n",
    "    q = deque(ids_to_start)\n",
    "    submitted: set[str] = set()\n",
    "\n",
    "    while q:\n",
    "        df = get_instances()\n",
    "        running = running_instance_ids(df)\n",
    "        known = set(df[\"PartitionKey\"].astype(str).tolist()) if len(df) else set()\n",
    "\n",
    "        capacity = max_concurrent - len(running)\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        print(f\"[{now}] running={len(running)} capacity={capacity} remaining_to_submit={len(q)}\")\n",
    "\n",
    "        if capacity <= 0:\n",
    "            time.sleep(poll_seconds)\n",
    "            continue\n",
    "\n",
    "        started_now = 0\n",
    "        while started_now < capacity and q:\n",
    "            audience_id = q.popleft()\n",
    "\n",
    "            # Skip if it already exists (table caught up) or we already submitted it\n",
    "            if audience_id in submitted or audience_id in known:\n",
    "                continue\n",
    "\n",
    "            resp = durable_start(audience_id, force=1)\n",
    "            submitted.add(audience_id)\n",
    "            started_now += 1\n",
    "\n",
    "            if resp is None:\n",
    "                print(f\"Submitted {audience_id}\")\n",
    "            elif resp.status_code >= 400:\n",
    "                print(f\"  ! start failed for {audience_id}: {resp.status_code} {resp.text[:200]}\")\n",
    "            else:\n",
    "                print(f\"Submitted {audience_id}: {resp.status_code}\")\n",
    "\n",
    "        time.sleep(poll_seconds)\n",
    "\n",
    "    print(f\"Done. Submitted {len(submitted)} audience(s).\")\n",
    "    return submitted\n",
    "\n",
    "ids_to_start = sorted(missing_instance_audiences)\n",
    "print(f\"Starting {len(ids_to_start)} missing audience(s) with max {MAX_CONCURRENT} running...\")\n",
    "submitted_missing = submit_missing_with_concurrency(ids_to_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restart_due",
   "metadata": {},
   "source": [
    "# Restart due instances (optional) with the same concurrency guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restart_due_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_next_run(custom_status: Any) -> datetime | None:\n",
    "    if not isinstance(custom_status, dict):\n",
    "        return None\n",
    "    nr = custom_status.get(\"next_run\")\n",
    "    if not isinstance(nr, str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(nr)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.to_datetime(nr, utc=True).to_pydatetime()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def restart_with_concurrency(\n",
    "    ids_to_restart: list[str],\n",
    "    max_concurrent: int = MAX_CONCURRENT,\n",
    "    poll_seconds: int = POLL_SECONDS,\n",
    ") -> set[str]:\n",
    "    q = deque(ids_to_restart)\n",
    "    restarted: set[str] = set()\n",
    "\n",
    "    while q:\n",
    "        df = get_instances()\n",
    "        running = running_instance_ids(df)\n",
    "        capacity = max_concurrent - len(running)\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        print(f\"[{now}] running={len(running)} capacity={capacity} remaining_to_restart={len(q)}\")\n",
    "\n",
    "        if capacity <= 0:\n",
    "            time.sleep(poll_seconds)\n",
    "            continue\n",
    "\n",
    "        restarted_now = 0\n",
    "        while restarted_now < capacity and q:\n",
    "            instance_id = q.popleft()\n",
    "\n",
    "            # Don't touch anything currently running\n",
    "            if instance_id in running:\n",
    "                continue\n",
    "\n",
    "            del_resp = durable_delete(instance_id)\n",
    "            if del_resp is not None and del_resp.status_code >= 400:\n",
    "                print(f\"  ! delete failed for {instance_id}: {del_resp.status_code} {del_resp.text[:200]}\")\n",
    "                continue\n",
    "\n",
    "            start_resp = durable_start(instance_id, force=1)\n",
    "            restarted.add(instance_id)\n",
    "            restarted_now += 1\n",
    "\n",
    "            if start_resp is None:\n",
    "                print(f\"Restarted {instance_id}\")\n",
    "            elif start_resp.status_code >= 400:\n",
    "                print(f\"  ! restart start failed for {instance_id}: {start_resp.status_code} {start_resp.text[:200]}\")\n",
    "            else:\n",
    "                print(f\"Restarted {instance_id}: {start_resp.status_code}\")\n",
    "\n",
    "        time.sleep(poll_seconds)\n",
    "\n",
    "    print(f\"Done. Restarted {len(restarted)} instance(s).\")\n",
    "    return restarted\n",
    "\n",
    "# Identify due instances\n",
    "instances_now = get_instances()\n",
    "if instances_now.empty:\n",
    "    print(\"No instances found.\")\n",
    "else:\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    instances_now[\"next_run_dt\"] = instances_now[\"CustomStatus\"].apply(_parse_next_run)\n",
    "    instances_now[\"state\"] = instances_now[\"CustomStatus\"].apply(lambda cs: cs.get(\"state\") if isinstance(cs, dict) else None)\n",
    "    instances_now[\"is_running\"] = instances_now.apply(is_running_row, axis=1)\n",
    "\n",
    "    due = instances_now.loc[\n",
    "        instances_now[\"next_run_dt\"].notna() & (instances_now[\"next_run_dt\"] <= now_utc)\n",
    "    ].copy().sort_values(\"next_run_dt\")\n",
    "\n",
    "    print(f\"Checked {len(instances_now)} instance(s) at {now_utc.isoformat()}\")\n",
    "    print(f\"Found {len(due)} due instance(s)\")\n",
    "    # if not due.empty:\n",
    "    #     display(due[[\"PartitionKey\", \"RuntimeStatus\", \"state\", \"CompletedTime\", \"is_running\", \"next_run_dt\", \"CreatedTime\"]])\n",
    "\n",
    "    # Only restart due instances that are NOT running\n",
    "    due_not_running = due.loc[~due[\"is_running\"], \"PartitionKey\"].astype(str).tolist()\n",
    "    print(f\"Due + not running => {len(due_not_running)} instance(s) eligible to restart\")\n",
    "\n",
    "    # Uncomment to actually restart\n",
    "    restarted_due = restart_with_concurrency(due_not_running)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restart_error",
   "metadata": {},
   "source": [
    "# Restart errored instances (CustomStatus.state == \"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restart_error_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_now = get_instances()\n",
    "if instances_now.empty:\n",
    "    print(\"No instances found.\")\n",
    "else:\n",
    "    instances_now[\"state\"] = instances_now[\"CustomStatus\"].apply(\n",
    "        lambda cs: cs.get(\"state\") if isinstance(cs, dict) else None\n",
    "    )\n",
    "    instances_now[\"is_running\"] = instances_now.apply(is_running_row, axis=1)\n",
    "\n",
    "    errored = instances_now.loc[(instances_now[\"state\"] == \"Error\") | instances_now[\"state\"].isna()].copy()\n",
    "\n",
    "    # Optional safety: only restart errored instances that are still active in the DB\n",
    "    if \"audience_ids\" in globals() and len(errored):\n",
    "        errored = errored.loc[errored[\"PartitionKey\"].astype(str).isin(audience_ids)].copy()\n",
    "\n",
    "    print(f\"Found {len(errored)} errored instance(s) (CustomStatus.state == 'Error')\")\n",
    "    # if not errored.empty:\n",
    "    #     display(\n",
    "    #         errored[[\"PartitionKey\", \"RuntimeStatus\", \"state\", \"CompletedTime\", \"is_running\", \"CreatedTime\"]]\n",
    "    #     )\n",
    "\n",
    "    # Only restart errored instances that are NOT running\n",
    "    errored_not_running = errored.loc[~errored[\"is_running\"], \"PartitionKey\"].astype(str).tolist()\n",
    "    print(f\"Error + not running => {len(errored_not_running)} instance(s) eligible to restart\")\n",
    "\n",
    "    # Uncomment to actually restart\n",
    "    restarted_error = restart_with_concurrency(errored_not_running)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
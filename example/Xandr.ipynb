{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set environmental variables for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "for k,v in json.load(open(\"local.settings.json\"))[\"Values\"].items():\n",
    "    os.environ[k] = v\n",
    "from libs.openapi.clients import XandrAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.data import register_binding, from_bind\n",
    "import pandas as pd\n",
    "\n",
    "if not from_bind(\"xandr_dashboard\"):\n",
    "    register_binding(\n",
    "        \"xandr_dashboard\",\n",
    "        \"Structured\",\n",
    "        \"sql\",\n",
    "        url=os.environ[\"DATABIND_SQL_XANDR\"],\n",
    "        schemas=[\"dashboard\"],\n",
    "    )\n",
    "provider = from_bind(\"xandr_dashboard\")\n",
    "last_creative = next(\n",
    "    iter(\n",
    "        next(\n",
    "            iter(\n",
    "                pd.read_sql(\n",
    "                    sql=\"SELECT MAX([last_modified]) FROM [dashboard].[creatives]\",\n",
    "                    con=provider.connect().connection(),\n",
    "                ).iloc\n",
    "            ),\n",
    "            \"1970-01-01 00:00:00\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "str(last_creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiopenapi3 import ResponseSchemaError\n",
    "XA = XandrAPI(asynchronus=False)\n",
    "factory = XA.createRequest(\"GetCreative\")\n",
    "try:\n",
    "    header, data, raw = factory.request(parameters={\"num_elements\": 100, \"start_element\": 0})\n",
    "except:\n",
    "    display(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XA = XandrAPI(asynchronus=False)\n",
    "start_element = 0\n",
    "num_elements = 100\n",
    "creatives = []\n",
    "while True:\n",
    "    print(f\"{start_element}\")\n",
    "    factory = XA.createRequest(\"GetCreative\")\n",
    "    response = factory(parameters={\"num_elements\": num_elements, \"start_element\": start_element})\n",
    "    if response.response.status == \"OK\":\n",
    "        start_element += response.response.num_elements\n",
    "        creatives += response.response.creatives\n",
    "        if start_element >= response.response.count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, hashlib, httpx, logging, orjson, os, pandas as pd, re\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from urllib.parse import urlparse, parse_qs, unquote_plus\n",
    "\n",
    "CACHE_DIR = \"xandr_docs_cache\"\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "\n",
    "def combine_dicts(dict_list):\n",
    "    # Function to combine multiple dictionaries\n",
    "    combined_dict = {}\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            if key not in combined_dict:\n",
    "                combined_dict[key] = []\n",
    "            combined_dict[key].extend(value)\n",
    "    return {k: list(set(v)) for k, v in combined_dict.items()}\n",
    "\n",
    "\n",
    "def transform_url(url):\n",
    "    # Function to replace spaces with underscores and angle brackets with curly brackets\n",
    "    def replace_match(match):\n",
    "        return \"{\" + match.group(1).replace(\" \", \"_\") + \"}\"\n",
    "\n",
    "    # Regular expression pattern to find text between angle brackets\n",
    "    pattern = r\"<([^>]+)>\"\n",
    "\n",
    "    # Replace the matches using the pattern and the replace_match function\n",
    "    transformed_url = re.sub(pattern, replace_match, url)\n",
    "    return transformed_url\n",
    "\n",
    "\n",
    "def replace_uppercase_words(text):\n",
    "    # Pattern explanation:\n",
    "    # \\b - word boundary\n",
    "    # [A-Z]+ - one or more uppercase letters\n",
    "    # (?:[-_][A-Z]+)+ - non-capturing group for dash/underscore followed by uppercase letters, repeated one or more times\n",
    "    pattern = r\"\\b[A-Z]+(?:[-_][A-Z]+)+\\b\"\n",
    "\n",
    "    # Function to replace dashes/underscores with underscores and add curly braces\n",
    "    def replace_with_underscores(match):\n",
    "        word = match.group(0)\n",
    "        return \"{\" + word.replace(\"-\", \"_\").replace(\"_\", \"_\") + \"}\"\n",
    "\n",
    "    # Replace all occurrences in the text\n",
    "    return re.sub(pattern, replace_with_underscores, text)\n",
    "\n",
    "\n",
    "def replace_camel_case(text):\n",
    "    # Regex pattern explanation:\n",
    "    # \\{ - Matches the opening curly brace\n",
    "    # ([a-z]+) - Matches and captures one or more lowercase letters\n",
    "    # ([A-Z]) - Matches and captures an uppercase letter\n",
    "    # ([a-zA-Z]*) - Matches and captures zero or more letters of any case\n",
    "    # \\} - Matches the closing curly brace\n",
    "    pattern = r\"\\{([a-z]+)([A-Z])([a-zA-Z]*)\\}\"\n",
    "\n",
    "    # Replacement function\n",
    "    def replace_with_underscore_and_uppercase(match):\n",
    "        return (\n",
    "            \"{\" + (match.group(1) + \"_\" + match.group(2) + match.group(3)).upper() + \"}\"\n",
    "        )\n",
    "\n",
    "    # Replace all occurrences in the text\n",
    "    return re.sub(pattern, replace_with_underscore_and_uppercase, text)\n",
    "\n",
    "\n",
    "def get_cache_filename(url):\n",
    "    # Use MD5 hash of the URL as the filename\n",
    "    return os.path.join(\n",
    "        CACHE_DIR, hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "    )\n",
    "\n",
    "\n",
    "async def fetch_url(client, url):\n",
    "    cache_file = get_cache_filename(url)\n",
    "    # Check if the URL is already cached\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    else:\n",
    "        response = await client.get(url, headers={\"accept\": \"application/json\"})\n",
    "        content = response.json()[\"html\"]\n",
    "        # Cache the response\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "        return content\n",
    "\n",
    "\n",
    "async def fetch_json(client, url, headers=None):\n",
    "    cache_file = get_cache_filename(url)\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            return orjson.loads(file.read())\n",
    "    else:\n",
    "        response = await client.get(url, headers=headers)\n",
    "        content = response.json()\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(orjson.dumps(content).decode(\"utf-8\"))\n",
    "        return content\n",
    "\n",
    "\n",
    "async def fetch_data():\n",
    "    async with httpx.AsyncClient(timeout=None) as client:\n",
    "        response = await fetch_json(\n",
    "            client,\n",
    "            url=\"https://xandr-be-prod.zoominsoftware.io/bundle/xandr-api/toc/api-getting-started.html\",\n",
    "            headers={\"accept\": \"application/json\"},\n",
    "        )\n",
    "        soup = BeautifulSoup(response[\"nav-54\"], \"html.parser\")\n",
    "        root = soup.find(\"ul\")\n",
    "\n",
    "        tasks = []\n",
    "        for ul in root.find_all(\"ul\"):\n",
    "            ref = ul.find(\"a\").text.strip()\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                service_name = li.find(\"a\").text.strip()\n",
    "                if ref != service_name:\n",
    "                    url = li.find(\"a\").get(\"href\")\n",
    "                    task = fetch_url(client, url)\n",
    "                    tasks.append((ref, service_name, task, url))\n",
    "\n",
    "        results = await asyncio.gather(*(task for _, _, task, _ in tasks))\n",
    "\n",
    "        all_ops = pd.DataFrame()\n",
    "        for (ref, service_name, task, url), result in zip(tasks, results):\n",
    "            if tables := BeautifulSoup(result, \"html.parser\").find_all(\"table\"):\n",
    "                for table in tables:\n",
    "                    df_api = pd.read_html(StringIO(str(table)))[0]\n",
    "                    if set(df_api.columns) == set(\n",
    "                        [\"HTTP Method\", \"Endpoint\", \"Description\"]\n",
    "                    ):\n",
    "                        df_api.rename(\n",
    "                            columns={\n",
    "                                \"HTTP Method\": \"method\",\n",
    "                                \"Description\": \"description\",\n",
    "                            },\n",
    "                            inplace=True,\n",
    "                        )\n",
    "                        df_api[\"Endpoint\"] = df_api[\"Endpoint\"].apply(\n",
    "                            lambda x: unquote_plus(\n",
    "                                re.sub(\n",
    "                                    r\"\\s+\",\n",
    "                                    \" \",\n",
    "                                    x.replace(\"(\", \" (\")\n",
    "                                    .replace(\"/ \", \"/\")\n",
    "                                    .replace(\"http\", \"https\")\n",
    "                                    .replace(\"httpss\", \"https\")\n",
    "                                    .replace(\".com\", \".com/\")\n",
    "                                    .replace(\".com//\", \".com/\")\n",
    "                                    .replace(\"- \", \"-\")\n",
    "                                    .replace(\" ?\", \"?\")\n",
    "                                    .replace(\"POST\", \" POST\")\n",
    "                                    .replace(\"Important:\", \" Important:\")\n",
    "                                    .replace(\"Note:\", \" Note:\")\n",
    "                                    .replace(\"Tip:\", \" Tip:\")\n",
    "                                    .replace(\"Warning:\", \" Warning:\"),\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                        df_api[\"External Documentation\"] = url\n",
    "                        for _, record in df_api.iterrows():\n",
    "                            for t in [\"Important\", \"Note\", \"Tip\", \"Warning\"]:\n",
    "                                if len(msg := record[\"Endpoint\"].split(f\" {t}:\")) > 1:\n",
    "                                    record[\"description\"] += f\"\\n{t}:{msg[-1]}\"\n",
    "                        for _, record in df_api.iterrows():\n",
    "                            if (\n",
    "                                len(endpoints := record[\"Endpoint\"].split(\"https://\"))\n",
    "                                > 1\n",
    "                            ):\n",
    "                                r = dict(record)\n",
    "                                df_api = pd.concat(\n",
    "                                    [\n",
    "                                        df_api,\n",
    "                                        pd.DataFrame(\n",
    "                                            [\n",
    "                                                {\n",
    "                                                    **r,\n",
    "                                                    \"Endpoint\": f\"https://{e}\".strip(),\n",
    "                                                }\n",
    "                                                for e in set(endpoints)\n",
    "                                                if e\n",
    "                                            ]\n",
    "                                        ),\n",
    "                                    ],\n",
    "                                    ignore_index=True,\n",
    "                                )\n",
    "\n",
    "                        for index, record in df_api.iterrows():\n",
    "                            if len(endpoints := record[\"Endpoint\"].split(\"http\")) > 2:\n",
    "                                df_api.drop(index, inplace=True)\n",
    "                        df_api.drop_duplicates([\"Endpoint\"], keep=\"first\", inplace=True)\n",
    "                        df_api[\"requestBody\"] = df_api[\"Endpoint\"].apply(\n",
    "                            lambda x: r[-1].split(\")\")[0].replace(\" JSON\", \"\")\n",
    "                            if len(\n",
    "                                r := x.replace(\"{\", \"(\")\n",
    "                                .replace(\"}\", \")\")\n",
    "                                .split(\" POST\")[0]\n",
    "                                .strip()\n",
    "                                .split(\" (\")\n",
    "                            )\n",
    "                            > 1\n",
    "                            else None\n",
    "                        )\n",
    "                        df_api[\"url\"] = df_api[\"Endpoint\"].apply(\n",
    "                            lambda x: urlparse(\n",
    "                                transform_url(x.split(\" (\")[0].strip())\n",
    "                                .split(\" \")[0]\n",
    "                                .strip()\n",
    "                            )\n",
    "                        )\n",
    "                        df_api[\"parameters\"] = df_api[\"url\"].apply(\n",
    "                            lambda x: {\n",
    "                                k: [\n",
    "                                    p.upper().replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                                    for p in v\n",
    "                                ]\n",
    "                                for k, v in parse_qs(x.query).items()\n",
    "                            }\n",
    "                        )\n",
    "                        df_api[\"path\"] = df_api[\"url\"].apply(\n",
    "                            lambda x: replace_camel_case(\n",
    "                                replace_uppercase_words(\n",
    "                                    unquote_plus(x.path)\n",
    "                                    .replace(\"[\", \"{\")\n",
    "                                    .replace(\"]\", \"}\")\n",
    "                                )\n",
    "                                .replace(\"{{\", \"{\")\n",
    "                                .replace(\"}}\", \"}\")\n",
    "                            )\n",
    "                        )\n",
    "                        df_api[\"server\"] = df_api[\"url\"].apply(lambda x: x.netloc)\n",
    "                        df_api[\"tags\"] = ref\n",
    "                        df_api[\"tags\"] = df_api[\"tags\"].apply(\n",
    "                            lambda x: [x, service_name]\n",
    "                        )\n",
    "                        all_ops = pd.concat(\n",
    "                            [all_ops, df_api[~df_api[\"path\"].str.endswith(\"/meta\")]]\n",
    "                        )\n",
    "\n",
    "        all_ops.drop_duplicates([\"Endpoint\"], keep=\"first\", inplace=True)\n",
    "        paths = {\n",
    "            path: {\n",
    "                method.lower(): {\n",
    "                    \"description\": \"\\n\".join(set(method_df[\"description\"].to_list()))\n",
    "                    .replace(\"Note:\", \"\\nNote:\")\n",
    "                    .replace(\"  \", \" \")\n",
    "                    .replace(\"\\n\\n\", \"\\n\")\n",
    "                    + \"\\n\"\n",
    "                    + \"\\n\".join(set(method_df[\"Endpoint\"].to_list())),\n",
    "                    \"parameters\": [\n",
    "                        #     {\n",
    "                        #         \"name\": k,\n",
    "                        #         \"in\": \"query\",\n",
    "                        #         \"schema\": {\"type\": \"string\"},\n",
    "                        #         \"examples\": {\n",
    "                        #             str(i): ev.replace(\"ID_VALUE\", \"ID\")\n",
    "                        #             for i, ev in enumerate(v)\n",
    "                        #         },\n",
    "                        #     }\n",
    "                        #     for k, v in combine_dicts(\n",
    "                        #         [p for p in method_df[\"parameters\"].to_list() if p]\n",
    "                        #     ).items()\n",
    "                    ],\n",
    "                    \"requestBody\": list(set([rb.replace(\" \", \"-\") for rb in rbs if rb]))\n",
    "                    if len(rbs := method_df[\"requestBody\"].to_list())\n",
    "                    else None,\n",
    "                    \"externalDocs\": {\n",
    "                        \"url\": method_df[\"External Documentation\"].to_list()[0]\n",
    "                    },\n",
    "                }\n",
    "                for method, method_df in path_df.groupby(\"method\")\n",
    "            }\n",
    "            for path, path_df in all_ops.groupby(\"path\")\n",
    "        }\n",
    "\n",
    "        refs = []\n",
    "        for path, op in paths.items():\n",
    "            refs += [o[\"requestBody\"] for o in op.values() if o[\"requestBody\"]]\n",
    "            op[\"parameters\"] = [\n",
    "                {\n",
    "                    \"name\": k,\n",
    "                    \"in\": \"query\",\n",
    "                    \"schema\": {\"type\": \"string\"},\n",
    "                    \"required\": True,\n",
    "                }\n",
    "                for k in re.findall(r\"\\{([^}]+)\\}\", path)\n",
    "            ]\n",
    "\n",
    "        schemas = {}\n",
    "        tasks = []\n",
    "\n",
    "        for path in paths.keys():\n",
    "            if len(path.split(\"/\")) == 2:\n",
    "                url = f\"https://api.appnexus.com{path}/meta\"\n",
    "                headers = {\n",
    "                    \"accept\": \"application/json\",\n",
    "                    \"authorization\": \"Bearer authn:279514:ac836e26876a3:nym2\"\n",
    "                }\n",
    "                tasks.append((fetch_json(client, url, headers), path))\n",
    "\n",
    "        for (task, path) in tasks:\n",
    "            try:\n",
    "                result = await task\n",
    "                if f := result.get(\"response\").get(\"fields\"):\n",
    "                    schemas[path[1:]] = f\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching schema for {path}: {e}\")\n",
    "\n",
    "        return {\"paths\": paths, \"schemas\": schemas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_type_to_openapi(field):\n",
    "    \"\"\"\n",
    "    Maps the field type from the Xandr API to the corresponding OpenAPI data type.\n",
    "    \"\"\"\n",
    "    type_mapping = {\n",
    "        'int': ('integer', None),\n",
    "        'double': ('number', 'double'),\n",
    "        'date': ('string', 'date-time'),\n",
    "        'money': ('number', 'float'),\n",
    "        'string': ('string', None),\n",
    "        'boolean': ('boolean', None)\n",
    "    }\n",
    "    openapi_type, format = type_mapping.get(field['type'], ('string', None))\n",
    "    return openapi_type, format\n",
    "\n",
    "def process_schema_fields(fields):\n",
    "    \"\"\"\n",
    "    Processes each field in the schema to generate the OpenAPI schema.\n",
    "    \"\"\"\n",
    "    openapi_fields = {}\n",
    "    for field in fields:\n",
    "        if field['type'] == 'array of objects':\n",
    "            if len(field.get('fields', [])) == 1:\n",
    "                # Array, but schema is not an object, instead it is the type of the single item in fields\n",
    "                item_type, item_format = map_type_to_openapi(field['fields'][0])\n",
    "                openapi_fields[field['name']] = {\n",
    "                    'type': 'array',\n",
    "                    'items': {'type': item_type}\n",
    "                }\n",
    "                if item_format:\n",
    "                    openapi_fields[field['name']]['items']['format'] = item_format\n",
    "            else:\n",
    "                # Array of objects, needs recursive processing\n",
    "                openapi_fields[field['name']] = {\n",
    "                    'type': 'array',\n",
    "                    'items': {'type': 'object', 'properties': process_schema_fields(field.get('fields', []))}\n",
    "                }\n",
    "        else:\n",
    "            # Regular field\n",
    "            openapi_type, format = map_type_to_openapi(field)\n",
    "            openapi_fields[field['name']] = {'type': openapi_type}\n",
    "            if format:\n",
    "                openapi_fields[field['name']]['format'] = format\n",
    "    return openapi_fields\n",
    "\n",
    "def generate_openapi_schemas(schemas):\n",
    "    \"\"\"\n",
    "    Generates OpenAPI schemas for each schema in the Xandr JSON file.\n",
    "    \"\"\"\n",
    "    openapi_schemas = {}\n",
    "    for schema_name, fields in schemas.items():\n",
    "        openapi_schemas[schema_name] = {\n",
    "            'type': 'object',\n",
    "            'properties': process_schema_fields(fields)\n",
    "        }\n",
    "    return openapi_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xandr.json\", \"wb\") as f:\n",
    "    f.write(\n",
    "        orjson.dumps(\n",
    "            # generate_openapi_schemas(\n",
    "                await fetch_data(),\n",
    "            # )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(CACHE_DIR, hashlib.md5(\"https://xandr-be-prod.zoominsoftware.io/bundle/xandr-api/toc/api-getting-started.html\".encode()).hexdigest())) as file:\n",
    "    display(list(json.load(file).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attached JSON file represents \"paths\" and \"schemas\" for the Xandr API. Each path may have a \"post\", \"get\", \"put\", and/or \"delete\" key which represents the operations that can be executed on that path, and may have a \"parameters\" key which represents parameters that are used with all of the operations for that path. \n",
    "\n",
    "The \"post\" key represents an HTTP POST request for creating a new record.\n",
    "The \"get\" key represents an HTTP GET request for reading one or more existing records.\n",
    "The \"put\" key represents an HTTP PUT request for updating an existing record.\n",
    "The \"delete\" key represents an HTTP DELETE request for deleting an existing record.\n",
    "\n",
    "Each operation will have a \"description\", \"parameters\", \"requestBody\", and \"externalDocs\" keys.\n",
    "\n",
    "Each key in \"schemas\" (at the root level of the JSON file) is a list of fields for the object type that a given path manages. Each field has a \"name\", and \"type\" key, and may have a \"sort_by\", \"filter_by\", and/or \"read_only\" key.\n",
    "\n",
    "Fields that are of the type \"int\", \"double\", \"date\", or \"money\" can be filtered by min and max. For example:\n",
    "/campaign?min_id=47\n",
    "/campaign?min_advertiser_id=20\n",
    "\n",
    "Fields of the type \"date\" can be filtered by nmin and nmax as well. The nmin filter lets you find dates that are either null or after the specified date, and the nmax filter lets you find dates that are either null or before the specified date. For example:\n",
    "\n",
    "/campaign?nmax_start_date=2012-12-20+00:00:00\n",
    "/campaign?nmin_end_date=2013-01-01+00:00:00\n",
    "Note the required date/time syntax in the preceding example: YYYY-MM-DD+HH:MM:SS\n",
    "\n",
    "The following additional field-based filters on API responses:\n",
    "not_*\n",
    "like_*\n",
    "min_*\n",
    "max_*\n",
    "nmin_*\n",
    "nmax_*\n",
    "having_*\n",
    "having_min_*\n",
    "having_max_*\n",
    "Example:\n",
    "/placement?like_parent_brand_name=Outback\n",
    "\n",
    "Some services support search as a query string parameter to look for ID or name. \n",
    "For example:\n",
    "/placement?search=17\n",
    "\n",
    "To sort use the sort query string parameter and pass in a list of fields you'd like to sort by and whether you want them ascending (asc) or descending (desc). \n",
    "For example:\n",
    "/campaign?advertiser_id=1&sort=id.desc'\n",
    "\n",
    "When getting multiple records, pagination can be used with the start_element and num_elements parameters. If num_elements is not supplied, it defaults to 100 (which is also the maximum value).\n",
    "/campaign?start_element=20&num_elements=10\n",
    "\n",
    "By including append=true in the query string of a PUT call, a user can update only a particular child object instead of replacing all child objects. In other words, rather than overwriting an entire array with a new one on a PUT call, you can use append=true on the query string to add a single element to a long array.\n",
    "\n",
    "Write a python script that will use the attached JSON file and generate an OpenAPI v3.1.0 compliant component schema and parameter for each schemas defined in the JSON file.\n",
    "\n",
    "Note, OpenAPI only supports the following data types: \n",
    "string\n",
    "number\n",
    "integer\n",
    "boolean\n",
    "array\n",
    "object\n",
    "\n",
    "So, mapping the field types to OpenAPI data types with proper formatting would be prudent. For example, if the field type is date, the corresponding OpenAPI data type would be \"string\" with a format of \"date-time\".\n",
    "\n",
    "Also, fields that have the type \"array of objects\" will also have the key \"fields\" which represents another object type. In this case fields should be an array of the type \"object\" and the \"fields\" will need to be processed recursively to generate a proper OpenAPI schema. \n",
    "\n",
    "NOTE: If the field's type is \"array of objects\" and there is only one item in the \"fields\" key, then the field's type should be \"array\", but the schema should not be of the type \"object\". Instead, the schema should be whatever the type is of the one item in \"fields\" is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
